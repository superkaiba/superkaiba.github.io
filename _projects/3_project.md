---
layout: page
title: Multi-headed Self-attention Block
description: Implementation of a multi-headed self-attention block using only basic PyTorch for use in a vision transformer trained on CIFAR-10
img:assets/img/attention.png 
importance: 3
redirect: https://github.com/superkaiba/rep-learning-projects/blob/main/vision-transformer.py
category: Machine Learning
---

