{
  "about_config": {
    "current_paragraph": "Hey! I'm a PhD student in artificial intelligence at <a href='https://mila.quebec/en/' target='_blank'><strong>Mila</strong></a> and <a href='https://www.umontreal.ca/' target='_blank'><strong>Université de Montréal</strong></a> co-supervised by <a href='https://yoshuabengio.org/' target='_blank'><strong>Yoshua Bengio</strong></a> and <a href='https://www.guillaumelajoie.com/' target='_blank'><strong>Guillaume Lajoie</strong></a> (and previously <a href='https://cs.mcgill.ca/~dprecup/' target='_blank'><strong>Doina Precup</strong></a>). My research has been supported by <a href='https://vanier.gc.ca/en/home-accueil.html' target='_blank'><strong>Vanier</strong></a>, <a href='https://www.nserc-crsng.gc.ca/index_eng.asp' target='_blank'><strong>NSERC</strong></a>, and <a href='http://www.frqnt.gouv.qc.ca/en/' target='_blank'><strong>FRQNT</strong></a> scholarships.",
    "startup_paragraph": "",
    "current_interests_list": [
      "<strong>Agentic AI:</strong> in-context learning, continually learning agents, multi-agent architectures, long-term memory, open-ended learning, reasoning",
      "<strong>Model internals:</strong> mechanistic interpretability, compositionality, latent reasoning",
      "<strong>AI Safety:</strong> chain of thought monitoring",
      "<strong>Bio-inspired methods:</strong> neuroAI, evolutionary methods",
      "<strong>Applications:</strong> biology, scientific discovery, creative arts"
    ],
    "previous_list": [
      "<strong>Research:</strong> <a href='https://www.anthropic.com/' target='_blank'>Anthropic</a> (mech interp), <a href='https://www.occam.ai/' target='_blank'>Occam AI</a> (LLM agents), <a href='https://waabi.ai/' target='_blank'>Waabi</a> (autonomous driving), <a href='https://vectorinstitute.ai/' target='_blank'>Vector Institute</a> (HVAC), with <a href='https://mila.quebec/en/person/blake-richards/' target='_blank'>Blake Richards</a> (neuroAI), with <a href='https://cs.mcgill.ca/~dprecup/' target='_blank'>Doina Precup</a> (model-based RL)",
      "<strong>Software development:</strong> <a href='https://aws.amazon.com/' target='_blank'>Amazon</a>, <a href='https://www.expedia.com/' target='_blank'>Expedia</a>, <a href='https://www.square-enix.com/' target='_blank'>Square Enix</a>",
      "<strong>Entrepreneurship:</strong> <a href='https://lucentcell.bio/' target='_blank'>Lucent Cell</a> (CSO & Co-founder, Declined YC W26) – AI interpretability for drug discovery",
      "<strong>Awards:</strong> <a href='https://www.heidelberg-laureate-forum.org/' target='_blank'>Heidelberg Laureate Forum</a> (200 most promising young researchers), <a href='https://www.uoft.ai/projectx-2021' target='_blank'>Project X</a> winner"
    ],
    "research_goal_paragraph": "",
    "email": "thomasjiralerspong@gmail.com",
    "cta": "Check out my featured projects below",
    "personal_interests": "Outside of research, I enjoy making music, traveling the world, and playing strategy/deception games.",
    "old_current": [
      {
        "type": "position",
        "ref_type": "research_experience",
        "ref_id": "anthropic-2025"
      },
      {
        "type": "education",
        "ref_type": "education",
        "ref_id": "udem-phd"
      },
      {
        "type": "award",
        "ref_type": "awards",
        "ref_id": "vanier-2025"
      },
      {
        "type": "research_interests",
        "interests": [
          {
            "name": "Mechanistic interpretability",
            "papers": ["crosscoders-2026"]
          },
          {
            "name": "Compositionality",
            "papers": ["complexity-compositionality-icml2025", "geometric-compositionality-acl2025"]
          },
          {
            "name": "Neuroscience-inspired AI",
            "papers": ["random-weights-iclr2025"]
          }
        ]
      }
    ],
    "previous": [
      {
        "type": "award",
        "ref_type": "awards",
        "ref_id": "heidelberg-2023"
      },
      {
        "type": "award",
        "ref_type": "awards",
        "ref_id": "project-x-winner-2022"
      },
      {
        "type": "position",
        "ref_type": "research_experience",
        "ref_id": "occam-2024"
      },
      {
        "type": "position",
        "ref_type": "research_experience",
        "ref_id": "waabi-2023"
      },
      {
        "type": "position",
        "ref_type": "research_experience",
        "ref_id": "blake-richards-2022"
      },
      {
        "type": "position",
        "ref_type": "research_experience",
        "ref_id": "doina-precup-2022"
      },
      {
        "type": "service",
        "ref_type": "service",
        "ref_id": "mila-lab-rep-2023"
      },
      {
        "type": "service",
        "ref_type": "service",
        "ref_id": "mcgill-ai-pm-2021"
      },
      {
        "type": "education",
        "ref_type": "education",
        "ref_id": "mcgill-bsc"
      },
      {
        "type": "past_work_areas",
        "areas": [
          {
            "name": "Reinforcement learning",
            "papers": ["contrastive-retrospection-neurips2023", "ventilation-aaai2023", "forecaster-neurips2023", "ventilation-rldm2022"]
          },
          {
            "name": "Healthcare ML",
            "papers": ["ventilation-aaai2023", "ventilation-rldm2022"]
          },
          {
            "name": "Causal inference",
            "papers": ["llm-causal-iclr2024"]
          },
          {
            "name": "Diffusion models",
            "papers": ["diffusion-noise-2026", "frequency-noise-delta2025"]
          }
        ]
      }
    ]
  },
  "news": [
    {
      "id": "news-acl-2025",
      "date": "January 2025",
      "title": "Paper accepted to ACL 2025 with Highlight Award",
      "content": "Our paper \"Geometric Signatures of Compositionality Across a Language Model's Lifetime\" was accepted to ACL 2025 and received a Highlight Award (top ~2.5% of submissions)!"
    },
    {
      "id": "news-icml-2025",
      "date": "January 2025",
      "title": "Paper accepted to ICML 2025",
      "content": "Our work \"A Complexity-Based Theory of Compositionality\" was accepted to ICML 2025. We develop a theoretical framework connecting algorithmic complexity with compositional learning in neural networks."
    },
    {
      "id": "news-vanier-2025",
      "date": "May 2024",
      "title": "Awarded Vanier Canada Graduate Scholarship",
      "content": "Honored to receive the Vanier Canada Graduate Scholarship ($150,000), Canada's most prestigious doctoral award. Grateful for this support to continue my research on mechanistic interpretability and compositionality."
    },
    {
      "id": "news-anthropic-2025",
      "date": "April 2025",
      "title": "Starting as AI Research Fellow at Anthropic",
      "content": "Excited to join Anthropic as an AI Research Fellow working on mechanistic interpretability to discover behavioral differences between models."
    }
  ],
  "publications": {
    "conferences": [
      {
        "id": "crosscoders-2026",
        "highlight": true,
        "featured": true,
        "title": "Cross-Architecture Model Diffing With Crosscoders",
        "authors": ["T. Jiralerspong", "T. Bricken"],
        "venue": "Under Review at ICLR 2026",
        "year": 2026,
        "summary": "We provide the first application of crosscoders to cross-architecture model diffing, discovering features like CCP alignment in Qwen and American exceptionalism in Llama.",
        "abstract": "Model diffing, the process of comparing models' internal representations to identify their differences, is a promising approach for uncovering safety-critical behaviors in new models. However, its application has so far been primarily focused on comparing a base model with its finetune. Since new LLM releases are often novel architectures, cross-architecture methods are essential to make model diffing widely applicable. Crosscoders are one solution capable of cross-architecture model diffing but have only ever been applied to base vs finetune comparisons. We provide the first application of crosscoders to cross-architecture model diffing and introduce Dedicated Feature Crosscoders (DFCs), an architectural modification designed to better isolate features unique to one model. Using this technique, we find in an unsupervised fashion features including Chinese Communist Party alignment in Qwen3-8B and Deepseek-R1-0528-Qwen3-8B, American exceptionalism in Llama3.1-8B-Instruct, and copyright refusal in GPT-OSS-20B. Together, our results work towards establishing cross-architecture crosscoder model diffing as an effective method for identifying meaningful behavioral differences between AI models.",
        "link": "https://openreview.net/pdf?id=YXB8uigyOg",
        "image": "assets/images/papers/crosscoder-comparison_v1.png"
      },
      {
        "id": "diffusion-noise-2026",
        "highlight": false,
        "featured": true,
        "title": "Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise",
        "authors": ["L. Scimeca*", "T. Jiralerspong*", "B. Earnshaw", "J. Hartford", "Y. Bengio"],
        "venue": "Under Review at ICLR 2026",
        "year": 2026,
        "summary": "We introduce anisotropic noise operators that build inductive biases into diffusion models by manipulating frequency components, improving generative performance.",
        "abstract": "Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic covariances and show that, under full support, the learned score converges to the true data score as t → 0, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands.",
        "link": "https://arxiv.org/pdf/2510.09660v3",
        "image": "assets/images/papers/anisotropic.png"
      },
      {
        "id": "complexity-compositionality-icml2025",
        "highlight": true,
        "featured": true,
        "title": "A Complexity-Based Theory of Compositionality",
        "authors": ["Eric Elmoznino*", "Thomas Jiralerspong*", "Yoshua Bengio", "Guillaume Lajoie"],
        "venue": "ICML",
        "year": 2025,
        "summary": "We propose a formal mathematical definition of representational compositionality grounded in algorithmic information theory, validating it on synthetic and real-world data.",
        "abstract": "Compositionality is believed to be fundamental to intelligence. In humans, it underlies the structure of thought, language, and higher-level reasoning. In AI, compositional representations can enable a powerful form of out-of-distribution generalization, in which a model systematically adapts to novel combinations of known concepts. However, while we have strong intuitions about what compositionality is, we lack satisfying formal definitions for it that are measurable and mathematical. Here, we propose such a definition, which we call representational compositionality, that accounts for and extends our intuitions about compositionality. The definition is conceptually simple, quantitative, grounded in algorithmic information theory, and applicable to any representation. Intuitively, representational compositionality states that a compositional representation satisfies three properties. First, it must be expressive. Second, it must be possible to re-describe the representation as a function of discrete symbolic sequences with re-combinable parts, analogous to sentences in natural language. Third, the function that relates these symbolic sequences to the representation, analogous to semantics in natural language, must be simple. Through experiments on both synthetic and real world data, we validate our definition of compositionality and show how it unifies disparate intuitions from across the literature in both AI and cognitive science. We also show that representational compositionality, while theoretically intractable, can be readily estimated using standard deep learning tools.",
        "link": "https://arxiv.org/abs/2410.14817",
        "image": "assets/images/papers/complexity_compositionality.png"
      },
      {
        "id": "geometric-compositionality-acl2025",
        "highlight": true,
        "featured": true,
        "title": "Geometric Signatures of Compositionality Across a Language Model's Lifetime",
        "authors": ["Jin Hwa Lee*", "Thomas Jiralerspong*", "Lei Yu", "Emily Cheng"],
        "venue": "ACL",
        "year": 2025,
        "award": "Highlight Award",
        "summary": "We find that dataset compositionality is reflected in the intrinsic dimension of LM representations, with nonlinear and linear dimensionality encoding semantic vs superficial aspects.",
        "abstract": "By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.",
        "link": "https://aclanthology.org/2025.acl-long.265/",
        "image": "assets/images/papers/geometric_signatures_of_compositionality.png"
      },
      {
        "id": "random-weights-iclr2025",
        "highlight": false,
        "featured": true,
        "title": "Expressivity of Neural Networks with Random Weights and Learned Biases",
        "authors": ["Ezekiel Williams", "Alexandre Payeur", "Avery Hee-Woon Ryoo", "Thomas Jiralerspong", "Matthew G Perich", "Luca Mazzucato", "Guillaume Lajoie"],
        "venue": "ICLR",
        "year": 2025,
        "summary": "We prove feedforward networks with fixed random weights can approximate any continuous function by learning only biases, relevant for neuroscience and LLM fine-tuning.",
        "abstract": "Landmark universal function approximation results for neural networks with trained weights and biases provided the impetus for the ubiquitous use of neural networks as learning models in neuroscience and Artificial Intelligence (AI). Recent work has extended these results to networks in which a smaller subset of weights (e.g., output weights) are tuned, leaving other parameters random. However, it remains an open question whether universal approximation holds when only biases are learned, despite evidence from neuroscience and AI that biases significantly shape neural responses. The current paper answers this question. We provide theoretical and numerical evidence demonstrating that feedforward neural networks with fixed random weights can approximate any continuous function on compact sets. We further show an analogous result for the approximation of dynamical systems with recurrent neural networks. Our findings are relevant to neuroscience, where they demonstrate the potential for behaviourally relevant changes in dynamics without modifying synaptic weights, as well as for AI, where they shed light on recent fine-tuning methods for large language models, like bias and prefix-based approaches.",
        "link": "https://arxiv.org/pdf/2407.00957",
        "image": "assets/images/papers/expressivity_of_neural_networks.png"
      },
      {
        "id": "delta-ai-iclr2024",
        "highlight": false,
        "title": "Delta-AI: Local Objectives for Amortized Inference in Sparse Graphical Models",
        "authors": ["J. Falet", "H. Lee", "N. Malkin", "C. Sun", "D. Secrieru", "T. Jiralerspong", "D. Zhang", "G. Lajoie", "Y. Bengio"],
        "venue": "ICLR",
        "year": 2024,
        "summary": "We develop local objectives for amortized inference in sparse graphical models using delta functions",
        "link": "https://arxiv.org/pdf/2310.02423"
      },
      {
        "id": "contrastive-retrospection-neurips2023",
        "highlight": false,
        "title": "Contrastive Retrospection: honing in on critical steps for rapid learning and generalization in RL",
        "authors": ["C. Sun", "W. Yang", "T. Jiralerspong", "D. Malenfant", "B. Alsbury-Nealy", "Y. Bengio", "B. Richards"],
        "venue": "NeurIPS",
        "year": 2023,
        "summary": "We develop a contrastive learning method to identify critical states in RL that accelerate learning in sparse reward environments",
        "link": "https://arxiv.org/pdf/2210.05845.pdf"
      },
      {
        "id": "ventilation-aaai2023",
        "highlight": false,
        "featured": true,
        "title": "Towards Safe Mechanical Ventilation Treatment Using Deep Offline Reinforcement Learning",
        "authors": ["F. Kondrup*", "T. Jiralerspong*", "E. Lau", "N. de Lara", "J. Shkrob", "M.D. Tran", "D. Precup", "S. Basu"],
        "venue": "AAAI",
        "year": 2023,
        "summary": "We present DeepVent, a Conservative Q-Learning agent that learns optimal ventilator parameters to promote patient survival, outperforming physicians from MIMIC-III.",
        "abstract": "Mechanical ventilation is a key form of life support for patients with pulmonary impairment. Healthcare workers are required to continuously adjust ventilator settings for each patient, a challenging and time consuming task. Hence, it would be beneficial to develop an automated decision support tool to optimize ventilation treatment. We present DeepVent, a Conservative Q-Learning (CQL) based offline Deep Reinforcement Learning (DRL) agent that learns to predict the optimal ventilator parameters for a patient to promote 90 day survival. We design a clinically relevant intermediate reward that encourages continuous improvement of the patient vitals as well as addresses the challenge of sparse reward in RL. We find that DeepVent recommends ventilation parameters within safe ranges, as outlined in recent clinical trials. The CQL algorithm offers additional safety by mitigating the overestimation of the value estimates of out-of-distribution states/actions. We evaluate our agent using Fitted Q Evaluation (FQE) and demonstrate that it outperforms physicians from the MIMIC-III dataset.",
        "link": "https://ojs.aaai.org/index.php/AAAI/article/view/26862/26634",
        "image": "assets/images/papers/deepvent.png"
      },
      {
        "id": "hvac-uic2023",
        "highlight": false,
        "featured": false,
        "title": "A Comparison of Classical and Deep Reinforcement Learning Methods for HVAC Control",
        "authors": ["M. Wang", "J. Willes", "T. Jiralerspong", "M. Moezzi"],
        "venue": "UIC",
        "year": 2023,
        "summary": "We compare classical and deep RL approaches for HVAC control in building energy management",
        "link": "https://arxiv.org/pdf/2308.05711.pdf",
        "image": "assets/images/papers/hvac.png"
      }
    ],
    "workshops": [
      {
        "id": "frequency-noise-delta2025",
        "highlight": false,
        "featured": false,
        "title": "Shaping Inductive Bias in Diffusion Models through Frequency-Based Noise Control",
        "authors": ["T. Jiralerspong", "B. Earnshaw", "J. Hartford", "Y. Bengio", "L. Scimeca"],
        "venue": "ICLR Workshop on Deep Generative Models (DeLTa)",
        "year": 2025,
        "summary": "We devise frequency-based noising operators to purposefully set inductive biases in diffusion models for topologically structured data.",
        "abstract": "Diffusion Probabilistic Models (DPMs) are powerful generative models that have achieved unparalleled success in a number of generative tasks. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. For topologically structured data, we devise a frequency-based noising operator to purposefully manipulate, and set, these inductive biases. We first show that appropriate manipulations of the noising forward process can lead DPMs to focus on particular aspects of the distribution to learn. We show that different datasets necessitate different inductive biases, and that appropriate frequency-based noise control induces increased generative performance compared to standard diffusion. Finally, we demonstrate the possibility of ignoring information at particular frequencies while learning. We show this in an image corruption and recovery task, where we train a DPM to recover the original target distribution after severe noise corruption.",
        "link": "https://arxiv.org/abs/2501.17061",
        "image": "assets/images/papers/anisotropic.png"
      },
      {
        "id": "causal-imputation-neurips2024",
        "highlight": false,
        "title": "General Causal Imputation via Synthetic Interventions",
        "authors": ["M. Jiralerspong", "T. Jiralerspong", "V. Shah", "D. Sridhar", "G. Gidel"],
        "venue": "NeurIPS Workshop on Causal Representation Learning",
        "year": 2024,
        "summary": "We develop methods for causal imputation using synthetic interventions in observational data",
        "link": ""
      },
      {
        "id": "llm-causal-iclr2024",
        "highlight": false,
        "featured": true,
        "title": "Efficient Causal Graph Discovery Using Large Language Models",
        "authors": ["T. Jiralerspong*", "X. Chen*", "Y. More", "V. Shah", "Y. Bengio"],
        "venue": "ICLR Workshop on How Far Are We From AGI?",
        "year": 2024,
        "summary": "We propose a BFS-based framework using LLMs for causal graph discovery that requires only linear queries instead of quadratic, achieving state-of-the-art results.",
        "abstract": "We propose a novel framework that leverages LLMs for full causal graph discovery. While previous LLM-based methods have used a pairwise query approach, this requires a quadratic number of queries which quickly becomes impractical for larger causal graphs. In contrast, the proposed framework uses a breadth-first search (BFS) approach which allows it to use only a linear number of queries. We also show that the proposed method can easily incorporate observational data when available, to improve performance. In addition to being more time and data-efficient, the proposed framework achieves state-of-the-art results on real-world causal graphs of varying sizes. The results demonstrate the effectiveness and efficiency of the proposed method in discovering causal relationships, showcasing its potential for broad applicability in causal graph discovery tasks across different domains.",
        "link": "https://arxiv.org/pdf/2402.01207.pdf",
        "image": "assets/images/papers/efficient_causal_graph_discovery.png"
      },
      {
        "id": "forecaster-neurips2023",
        "highlight": false,
        "featured": true,
        "title": "Forecaster: Towards Temporally Abstract Tree-Search Planning from Pixels",
        "authors": ["T. Jiralerspong*", "F. Kondrup*", "D. Precup", "K. Khetarpal"],
        "venue": "NeurIPS Workshop on Generalization in Planning",
        "year": 2023,
        "summary": "We introduce Forecaster, a hierarchical RL approach that plans over high-level goals using a temporally abstract world model and tree-search from pixels.",
        "abstract": "The ability to plan at many different levels of abstraction enables agents to envision the long-term repercussions of their decisions and thus enables sample-efficient learning. This becomes particularly beneficial in complex environments from high-dimensional state space such as pixels, where the goal is distant and the reward sparse. We introduce Forecaster, a deep hierarchical reinforcement learning approach which plans over high-level goals leveraging a temporally abstract world model. Forecaster learns an abstract model of its environment by modelling the transitions dynamics at an abstract level and training a world model on such transition. It then uses this world model to choose optimal high-level goals through a tree-search planning procedure. It additionally trains a low-level policy that learns to reach those goals. Our method not only captures building world models with longer horizons, but also, planning with such models in downstream tasks. We empirically demonstrate Forecaster's potential in both single-task learning and generalization to new tasks in the AntMaze domain.",
        "link": "https://arxiv.org/pdf/2310.09997.pdf",
        "image": "assets/images/papers/forecaster.png"
      },
      {
        "id": "ventilation-rldm2022",
        "highlight": false,
        "featured": false,
        "title": "Deep Conservative Reinforcement Learning for Personalization of Mechanical Ventilation Treatment",
        "authors": ["F. Kondrup*", "T. Jiralerspong*", "E. Lau", "N. de Lara", "J. Shkrob", "M.D. Tran", "D. Precup", "S. Basu"],
        "venue": "Multi-disciplinary Conference on Reinforcement Learning and Decision Making (RLDM)",
        "year": 2022,
        "summary": "We apply conservative offline RL to personalize mechanical ventilation treatment in critical care, addressing distributional shift in medical settings",
        "link": "https://arxiv.org/pdf/2210.02552.pdf",
        "image": "assets/images/papers/deepvent.png"
      }
    ]
  },
  "research_experience": [
    {
      "id": "anthropic-2025",
      "highlight": true,
      "featured": true,
      "company": "Anthropic",
      "company_url": "https://www.anthropic.com/",
      "position": "AI Research Fellow",
      "location": "San Francisco, United States",
      "start_date": "2025-04",
      "end_date": "2025-10",
      "summary": "Working on mechanistic interpretability to discover behavioral differences between models",
      "details": "Conducting research on mechanistic interpretability and AI safety. Developing methods to understand how large language models represent and process information internally.",
      "mentors": [],
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "occam-2024",
      "highlight": false,
      "company": "Occam AI",
      "company_url": "https://www.occam.ai/",
      "position": "Research Scientist",
      "location": "New York City, United States",
      "start_date": "2024-06",
      "end_date": "2025-04",
      "summary": "Worked on optimization of interactions between networks of LLM agents and automated SQL query generation",
      "details": "Developed optimization algorithms for multi-agent LLM systems and created automated SQL query generation pipelines using large language models.",
      "mentors": [],
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "waabi-2023",
      "highlight": false,
      "company": "Waabi",
      "company_url": "https://waabi.ai/",
      "position": "Deep Learning Research Intern",
      "location": "Toronto, Canada",
      "start_date": "2023-06",
      "end_date": "2023-08",
      "summary": "Worked with Kelvin Wong and Chris Zhang on realistic and controllable traffic simulation using a transformer-based variational autoencoder",
      "details": "Developed realistic and controllable traffic simulation systems using transformer-based variational autoencoders for autonomous driving applications.",
      "mentors": ["Kelvin Wong", "Chris Zhang"],
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "blake-richards-2022",
      "highlight": false,
      "company": "Learning in Neural Circuits Lab, Mila/McGill University",
      "company_url": "https://mila.quebec/en/",
      "position": "Research Intern",
      "location": "Montreal, Canada",
      "start_date": "2022-09",
      "end_date": "2023-08",
      "summary": "Worked with Prof. Blake Richards on contrastive learning to discover critical states for reinforcement learning in sparse reward environments",
      "details": "Developed contrastive learning methods to identify critical states that enable rapid learning and generalization in reinforcement learning tasks with sparse rewards.",
      "mentors": ["Prof. Blake Richards"],
      "mentor_links": {"Prof. Blake Richards": "https://mila.quebec/en/person/blake-richards/"},
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "doina-precup-2022",
      "highlight": false,
      "company": "Reasoning and Learning Lab, Mila/McGill University",
      "company_url": "https://rl.cs.mcgill.ca/",
      "position": "Research Intern",
      "location": "Montreal, Canada",
      "start_date": "2022-01",
      "end_date": "2023-08",
      "summary": "Worked with Prof. Doina Precup on model-based reinforcement learning with affordance-aware tree-search planning directly from pixels",
      "details": "Developed model-based RL methods with affordance-aware tree-search planning that operate directly from pixel observations for complex decision-making tasks.",
      "mentors": ["Prof. Doina Precup"],
      "mentor_links": {"Prof. Doina Precup": "https://cs.mcgill.ca/~dprecup/"},
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "vector-2022",
      "highlight": false,
      "company": "Vector Institute for A.I.",
      "company_url": "https://vectorinstitute.ai/",
      "position": "Machine Learning Research Intern",
      "location": "Toronto, Canada",
      "start_date": "2022-09",
      "end_date": "2022-12",
      "summary": "Worked with John Willes and Marshall Wang on model-based reinforcement learning for HVAC control",
      "details": "Developed model-based reinforcement learning algorithms for optimizing HVAC systems in building energy management.",
      "mentors": ["John Willes", "Marshall Wang"],
      "technologies": ["PyTorch", "Python"]
    },
    {
      "id": "project-x-2021",
      "highlight": true,
      "featured": true,
      "company": "Project X, Machine Learning Research Competition",
      "company_url": "https://www.uoft.ai/projectx-2021",
      "position": "Co-leader of McGill's Team",
      "location": "Remote",
      "start_date": "2021-06",
      "end_date": "2022-02",
      "summary": "Led team to highest score out of 25 submitted papers on deep offline conservative RL for mechanical ventilation treatment",
      "details": "Co-led McGill's team in a machine learning research competition, receiving the highest score out of 25 submissions. Developed deep offline conservative reinforcement learning methods for mechanical ventilation treatment. Won $25,000 prize.",
      "mentors": [],
      "award": "Winner ($25,000 prize)",
      "technologies": ["PyTorch", "Python"]
    }
  ],
  "industry_experience": [
    {
      "id": "aws-2022",
      "highlight": false,
      "company": "Amazon Web Services (AWS) - S3 Team",
      "company_url": "https://aws.amazon.com/s3/",
      "position": "Software Development Engineer Intern",
      "location": "Vancouver, Canada",
      "start_date": "2022-05",
      "end_date": "2022-07",
      "summary": "Built JavaScript/Python tool to automate the incremental backup recovery system for AWS S3 (stores ~14 trillion objects)",
      "details": "Developed automation tools for the incremental backup recovery system supporting AWS S3, which stores approximately 14 trillion objects globally.",
      "technologies": ["JavaScript", "Python"]
    },
    {
      "id": "square-enix-2021",
      "highlight": false,
      "company": "Square Enix",
      "company_url": "https://www.square-enix.com/",
      "position": "Software Development Intern",
      "location": "Montreal, Canada",
      "start_date": "2021-05",
      "end_date": "2021-08",
      "summary": "Built localization system to allow a MOBA game to be translated into over 10 languages",
      "details": "Developed a comprehensive localization system enabling a MOBA game to be translated into over 10 languages, improving global accessibility.",
      "technologies": ["C#"]
    },
    {
      "id": "expedia-2019",
      "highlight": false,
      "company": "Expedia",
      "company_url": "https://www.expedia.com/",
      "position": "Software Development Intern",
      "location": "Montreal, Canada",
      "start_date": "2019-06",
      "end_date": "2019-08",
      "summary": "Built React/TypeScript tool to identify which elements of a webpage are broken and display them to developers",
      "details": "Developed a React/TypeScript debugging tool that automatically identifies broken webpage elements and provides convenient visualization for developers.",
      "technologies": ["JavaScript", "TypeScript", "React"]
    }
  ],
  "education": [
    {
      "id": "udem-phd",
      "highlight": true,
      "featured": true,
      "institution": "Université de Montréal",
      "institution_url": "https://www.umontreal.ca/",
      "degree": "PhD in Computer Science",
      "location": "Montreal, Canada",
      "start_date": "2023-09",
      "end_date": "2027-05",
      "status": "In Progress",
      "gpa": null,
      "thesis_topic": "Advancing our understanding of how neural networks learn and represent knowledge",
      "supervisors": ["Yoshua Bengio", "Guillaume Lajoie"],
      "supervisor_links": {
        "Yoshua Bengio": "https://yoshuabengio.org/",
        "Guillaume Lajoie": "https://www.guillaumelajoie.com/"
      },
      "awards": [
        "Vanier Canada Graduate Scholarship ($150,000)",
        "FRQNT Scholarship ($40,000) - Rank #1 among all applicants in category",
        "NSERC Canada Graduate Scholarship ($17,500)",
        "Hydro-Québec Excellence Scholarship ($10,000)",
        "Arbour Scholarship ($7,500)"
      ],
      "key_papers": ["complexity-compositionality-icml2025", "geometric-compositionality-acl2025"]
    },
    {
      "id": "mit-bmm-2024",
      "highlight": false,
      "institution": "Massachusetts Institute of Technology",
      "institution_url": "https://mit.edu/",
      "degree": "Brains, Minds, and Machines Summer Course",
      "location": "Cambridge, United States",
      "start_date": "2024",
      "end_date": "2024",
      "status": "Completed"
    },
    {
      "id": "mcgill-bsc",
      "highlight": false,
      "institution": "McGill University",
      "institution_url": "https://www.mcgill.ca/",
      "degree": "B.Sc., Honours Computer Science and Mathematics",
      "location": "Montreal, Canada",
      "start_date": "2020-09",
      "end_date": "2023-05",
      "status": "Completed",
      "gpa": "4.00/4.00",
      "supervisors": ["Blake Richards", "Doina Precup"],
      "supervisor_links": {
        "Blake Richards": "https://mila.quebec/en/person/blake-richards/",
        "Doina Precup": "https://cs.mcgill.ca/~dprecup/"
      },
      "notes": "Exchange semester at the National University of Singapore",
      "awards": [
        "J.W. McConnell Major Entrance Scholarship ($9,000)"
      ],
      "clubs": [
        "McGill AI Society - Technical Project Manager",
        "McGill NeuroTech - Machine Learning Developer",
        "McGill Robotics - Software Developer",
        "McGill Game Development Student Society"
      ],
      "undergrad_research": "Undergraduate research in reinforcement learning with Prof. Blake Richards and Prof. Doina Precup"
    }
  ],
  "awards": [
    {
      "id": "vanier-2025",
      "highlight": true,
      "featured": true,
      "title": "Vanier Canada Graduate Scholarship",
      "amount": "$150,000",
      "year": 2025,
      "description": "Canada's most prestigious doctoral scholarship",
      "link": "https://vanier.gc.ca/en/home-accueil.html"
    },
    {
      "id": "frqnt-2024",
      "highlight": true,
      "title": "FRQNT Master's Scholarship",
      "amount": "$40,000",
      "year": 2024,
      "description": "Rank #1 among all applicants in category",
      "link": "http://www.frqnt.gouv.qc.ca/en/"
    },
    {
      "id": "arbour-2024",
      "highlight": false,
      "title": "Arbour Scholarship",
      "amount": "$7,500",
      "year": 2024,
      "link": ""
    },
    {
      "id": "hydro-quebec-2024",
      "highlight": false,
      "title": "Hydro-Québec Excellence Scholarship",
      "amount": "$10,000",
      "year": 2024,
      "link": ""
    },
    {
      "id": "heidelberg-2023",
      "highlight": true,
      "featured": true,
      "title": "Heidelberg Laureate Forum",
      "year": 2023,
      "description": "Chosen as one of 200 most promising young researchers in math & CS worldwide",
      "link": "https://www.heidelberg-laureate-forum.org/"
    },
    {
      "id": "nserc-2023",
      "highlight": false,
      "title": "NSERC Canada Graduate Scholarship",
      "amount": "$17,500",
      "year": 2023,
      "link": "https://www.nserc-crsng.gc.ca/index_eng.asp"
    },
    {
      "id": "udem-masters-2023",
      "highlight": false,
      "title": "University of Montreal Master's Scholarship",
      "amount": "$5,000",
      "year": 2023,
      "link": ""
    },
    {
      "id": "mcgill-mobility-2022",
      "highlight": false,
      "title": "McGill Mobility Bursary for Exchanges",
      "amount": "$6,000",
      "year": 2022,
      "link": ""
    },
    {
      "id": "project-x-winner-2022",
      "highlight": true,
      "title": "Winner of UofT AI's Project X Competition",
      "amount": "$25,000",
      "year": 2022,
      "description": "Highest score out of 25 submitted papers",
      "link": "https://www.uoft.ai/projectx-2021"
    },
    {
      "id": "mcconnell-2020",
      "highlight": false,
      "title": "J.W. McConnell Major Entrance Scholarship",
      "amount": "$9,000",
      "year": 2020,
      "duration": "2020-2022",
      "link": ""
    },
    {
      "id": "cibpa-2021",
      "highlight": false,
      "title": "CIBPA Foundation Bursary",
      "amount": "$1,000",
      "year": 2021,
      "link": ""
    },
    {
      "id": "cibpa-2022",
      "highlight": false,
      "title": "CIBPA Foundation Bursary",
      "amount": "$2,500",
      "year": 2022,
      "link": ""
    },
    {
      "id": "cibpa-2023",
      "highlight": false,
      "title": "CIBPA Foundation Bursary",
      "amount": "$1,000",
      "year": 2023,
      "link": ""
    },
    {
      "id": "valedictorian-2020",
      "highlight": false,
      "title": "Marianopolis College Valedictorian",
      "year": 2020,
      "link": ""
    },
    {
      "id": "governor-general-2020",
      "highlight": false,
      "title": "Governor General of Canada's Academic Medal",
      "year": 2020,
      "link": ""
    }
  ],
  "teaching": [
    {
      "id": "udem-ta-2023",
      "highlight": false,
      "institution": "Université de Montréal",
      "institution_url": "https://www.umontreal.ca/",
      "role": "Teaching Assistant",
      "course": "Representation Learning",
      "year": 2023
    },
    {
      "id": "mcgill-ai-society-2021",
      "highlight": false,
      "institution": "McGill A.I. Society",
      "institution_url": "https://mcgillai.com/",
      "role": "Organizer/Teaching Assistant",
      "course": "Accelerated Intro to ML",
      "start_date": "2021",
      "end_date": "2023"
    },
    {
      "id": "mcgill-ta-2021",
      "highlight": false,
      "institution": "McGill University",
      "institution_url": "https://www.mcgill.ca/",
      "role": "Teaching Assistant",
      "course": "Software Systems",
      "start_date": "2021",
      "end_date": "2022"
    },
    {
      "id": "mcgill-lecturer-2022",
      "highlight": false,
      "institution": "McGill University",
      "institution_url": "https://www.mcgill.ca/",
      "role": "Guest Lecturer",
      "course": "Theory of Machine Learning",
      "year": 2022
    }
  ],
  "service": [
    {
      "id": "mila-lab-rep-2023",
      "highlight": false,
      "organization": "Mila",
      "organization_url": "https://mila.quebec/en/",
      "role": "Chairman of Lab Representatives",
      "start_date": "2023",
      "end_date": "Present"
    },
    {
      "id": "mila-social-2023",
      "highlight": false,
      "organization": "Mila",
      "organization_url": "https://mila.quebec/en/",
      "role": "Chairman of Social Committee",
      "start_date": "2023",
      "end_date": "Present"
    },
    {
      "id": "mila-recruitment-2023",
      "highlight": false,
      "organization": "Mila",
      "organization_url": "https://mila.quebec/en/",
      "role": "Executive Member of Recruitment Committee",
      "start_date": "2023",
      "end_date": "Present"
    },
    {
      "id": "mcgill-ai-advisor-2023",
      "highlight": false,
      "organization": "McGill AI Society",
      "organization_url": "https://mcgillai.com/",
      "role": "Senior Advisor",
      "start_date": "2023",
      "end_date": "Present"
    },
    {
      "id": "mcgill-ai-pm-2021",
      "highlight": false,
      "organization": "McGill AI Society",
      "organization_url": "https://mcgillai.com/",
      "role": "Technical Project Manager",
      "start_date": "2021",
      "end_date": "2023"
    },
    {
      "id": "main-conf-2022",
      "highlight": false,
      "organization": "Montreal AI & Neuroscience Conference",
      "organization_url": "",
      "role": "Organizer - Introduction to deep learning with PyTorch workshop",
      "year": 2022
    },
    {
      "id": "mcgill-neurotech-2021",
      "highlight": false,
      "organization": "McGill NeuroTech",
      "organization_url": "",
      "role": "Machine Learning Developer",
      "start_date": "2021",
      "end_date": "2022"
    },
    {
      "id": "mcgill-robotics-2020",
      "highlight": false,
      "organization": "McGill Robotics",
      "organization_url": "",
      "role": "Software Developer",
      "start_date": "2020",
      "end_date": "2021"
    }
  ],
  "press": [
    {
      "id": "scilogs-2024",
      "highlight": false,
      "outlet": "SciLogs",
      "title": "What Do Food and Research Have in Common? More Than You Might Think",
      "author": "Nina Beier",
      "date": "2024-01-24",
      "link": "https://scilogs.spektrum.de/hlf/what-do-food-and-research-have-in-common-more-than-you-might-think/",
      "summary": "Feature on my experience at the Heidelberg Laureate Forum"
    },
    {
      "id": "tribune-2022",
      "highlight": false,
      "outlet": "The McGill Tribune",
      "title": "Six McGill undergrads win UofT international artificial intelligence competition",
      "author": "Mikaela Shadick",
      "date": "2022-03-15",
      "link": "https://www.thetribune.ca/sci-tech/six-mcgill-undergrads-win-uoft-international-artificial-intelligence-competition-03152022/",
      "summary": "Coverage of Project X competition win"
    },
    {
      "id": "reporter-2022",
      "highlight": false,
      "outlet": "McGill Reporter",
      "title": "Undergrad team uses machine learning to create a better hospital ventilator",
      "author": "Richard Deschamps",
      "date": "2022-03-01",
      "link": "https://reporter.mcgill.ca/undergrad-team-uses-machine-learning-to-create-a-better-hospital-ventilator/",
      "summary": "Feature on mechanical ventilation research project"
    }
  ],
  "invited_talks": [
    {
      "id": "cucai-2022",
      "highlight": false,
      "event": "Canadian Undergraduate Conference on AI (CUCAI)",
      "year": 2022,
      "link": ""
    },
    {
      "id": "uoft-ai-2022",
      "highlight": false,
      "event": "University of Toronto AI Conference",
      "year": 2022,
      "link": ""
    },
    {
      "id": "mcgill-learnathon-2022",
      "highlight": false,
      "event": "McGill AI Society Learnathon",
      "year": 2022,
      "link": ""
    }
  ],
  "languages": [
    {
      "level": "Native",
      "languages": ["English", "French"]
    },
    {
      "level": "Advanced",
      "languages": ["Italian", "Spanish"]
    },
    {
      "level": "Beginner",
      "languages": ["Mandarin", "Japanese"]
    }
  ],
  "skills": {
    "programming": ["Python", "Java", "JavaScript", "R", "C", "C++", "C#", "OCaml", "SQL", "HTML", "CSS"],
    "ml_libraries": ["PyTorch", "TensorFlow", "Keras", "Pandas", "NumPy", "Matplotlib"],
    "tools": ["LaTeX", "Slurm", "Jupyter Notebooks", "Perforce", "GitHub", "Jira", "Unity"]
  }
}
